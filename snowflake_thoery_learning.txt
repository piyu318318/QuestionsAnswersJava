
CREATE A DATABASE IN SNOWFLAKE 
************************************************************************************************************************************************
-- Create a database from the share.
syntx :
create schema piyush_snowflake.my_schema;
use  schema piyush_snowflake.my_schema;

use database piyush_snowflake;
use schema my_schema;
create or replace table emp (
    id integer,
    name string,
    department string,
    salary number(10,2)
);


insert into emp (id, name, department, salary) values
(1, 'Alice', 'HR', 60000.00),
(2, 'Bob', 'Finance', 70000.00),
(3, 'Charlie', 'Engineering', 90000.00),
(4, 'Diana', 'Marketing', 65000.00),
(5, 'Ethan', 'Engineering', 85000.00),
(6, 'Fiona', 'Finance', 72000.00),
(7, 'George', 'HR', 58000.00),
(8, 'Hannah', 'Sales', 63000.00),
(9, 'Ian', 'Sales', 61000.00),
(10, 'Jane', 'Engineering', 95000.00);


-- Grant the PUBLIC role access to the database.
-- Optionally change the role name to restrict access to a subset of users.
grant imported privileges on database piyush_snowflake to role public;

Using grant command You are allowing all users (because of the PUBLIC role) to access the shared database and its objects, using the privileges that were imported from the provider.



1. Database 📦
A Database in Snowflake is like a big container.
It stores schemas, tables, views, functions, etc.
Think of a database as a library — it holds many collections (schemas).
You must create or use a database before you can create tables, views, etc.


2. Schema 🗂️
A Schema is a sub-container inside a database.
It organizes objects like tables, views, procedures, etc.
A database can have many schemas.

************************************************************************************************************************************************



SNOWFLAKE ARCHITECHTURE 
************************************************************************************************************************************************

There are three layesrs of snowflake 
1.cloud services - brain of the system . This is the layer where we manage the infra structure. manages security , metadata,query optimization this is all done in cloud services.
2.query processing (warehouses )- muscles of the systems because it provides the actual compute power to execute the querires. these are virtual computer resources that are used to process queries and all of the operation we are executing.These warehouses can also perforrm masisve parellel processing which makes them more efficient. we can like also combine multiple queries and process them at the same time.
3.storage - this is where actual data is getting stored , its hybrid columnar storage saved in blobs , the data is getting stored in columns so when we query the data we fetch the d those blobs , it makes more efficinent to store the data and for better performnace for analytical .



WAREHOUSES

warehouses are virtual compute instances of servers that are use to process the queries .
Warehoiuses provides the compute capacity 

sizes of warehouse 
xs- extra small - it has only one server (1 server)
s- small - if our queries are comples we can use small  (2 servers)
m- medium - if queries get more complex it would be good to fast the queries (4 servers)
L- large  -(8 servsers)
XL - (16 servsers)

MULTICLUSTERING  WAREHOUSE IN SNOWFLAKE 
Snowflake offers multi clustering this menas that one warehouse for example of size s contains the query load additional clusters can be activated and they can be clusetred togetther into one warehouse 

multicluster warehouse below 
______________________
|   small cluser █    |
|   small cluser █    |
|   small cluser █    |
_______________________


Create Warehouse using snowflake UI

To create a warehouse in snowflake you need to have admin access 
To create warehouse in snowflake we need accountadmin role and securityadmin role is second most powerful after accountadmin.
Sysadmin role usually manages the warehouses.
For every warehouse we can have multiple features a)auto suspend the warehouse if it is inactive for specific period to save the cost b)auto resume is the most recomendded feature 
when we execute the query the specific warehouse will start automatically so we dont need to turn on manually.
Another feature is multi cluster featuers you can simply increase and decrease the speciffc clusters while creating a warehouse.
While creating warehosue there are 2 types of warehosue 1)standerd warehouse - it is one we use in most cases 2)snowpark optimized warehouse - its litte different type of warehouse it offers the larger memory if we want to perform memory intension operations snowpark optimized warehouse comes in the picture, moslty used in machine learning.
query accelerator -> while creating the warehouse we can enable the query accelerator which will benifitd to fast the querires , also additionly if we enable thisit will cost more accoring to snowflake team plan.

create warehouse using SQL commandS.

CREATE OR REPLACE WAREHOUSE SECOND_WH
WITH
WAREHOUSE SIZE = XSMALL 
SCALING_POLICY = 'ECONOMY'
AUTO_SUSPEND = 300
MIN_CLUSTER_COUNT = 1
MAX_CLUSTER_COUNT = 2
INITIALLY_SUSPENDED = TRUE
COMMENT = 'THIS IS OUR FIRST WAREHOUSE'

We can also alter the warehouse using SQL commands bewlow 

1)ALTER WAREHOUSE SECOND_WH RESUME 
2)ALTER WAREHOUSE SECOND_WH SUSPEND
3)ALTER WAREHOUSE SECOND_WH RESUME SET  WAREHOUSE_SIZE = 'SMALL'
4)ALTER WAREHOUSE SECOND_WH RESUME SET  AUTO_SUSPEND = 60
5)DROP WAREHOUSE SECOND_WH

************************************************************************************************************************************************


SCALING POLICIES IN SNOWFLAKE 
************************************************************************************************************************************************
For exmaple we have multi cluster and at then same time on one warehouse use has executed multiple queries so to process multiple queries user will have to wait for long time so the solution is to scale up the depending on work dynamically on cluster in mulsti cluster warehouse. by using this we can redistribute the load of the queries on multi clusters.

use case of multi cluster  -> when we have more work load means we have more queries for example we have times when we have more users for exmaple we have peak hours in the afternoon then we have more users and more queries and thi is excly use case when we use multi cluster becasue in this case its very efficient for hanfling more workload so more queries and more users good use case is not when we have more complex queries.

use case of scaling up the cluster size : 
when we have more complex queries running on warehouse threfore we need more compute to increase the size of cluster instead of spinning up the addional cluster.


scaling policies types 

A)Standard Scaling Policy
Fast to add more compute power when needed.
starts the cluster immidiately when needed.
Fast to shut down extra compute when not needed.
Good for: workloads where you care about speed and performance more than cost.
Example: If many users suddenly run queries at once, Snowflake quickly adds more clusters to handle it.

B)Economy Scaling Policy
Slower to add or remove compute.
Prioritizes saving money over being super fast.
Good for: workloads where you’re okay with slower scaling to save costs.
Example: Snowflake waits a bit longer before adding new clusters even if queries pile up, hoping the load will go down soon and avoid paying for more clusters.

Super simple difference:
Standard = fast scaling for performance.
Economy = slower scaling for cost savings.
************************************************************************************************************************************************


TYPES OF DATABASES IN SNOWFLAKE 
************************************************************************************************************************************************

1. Standard (User-Created) Database
This is a regular database that you create to store your own data (tables, views, etc.).

2. System Databases
(Built-in by Snowflake, you don't create them)
SNOWFLAKE ➔ Holds metadata, billing, account usage info
SNOWFLAKE_SAMPLE_DATA ➔ Free public sample datasets (like TPCH_SF1).

3. Share Database (Shared via Data Sharing)
This is not your own — it’s shared with you from another Snowflake account.
You can't change the shared data, but you can query it like your own.
Used for: getting real-time data from partners/vendors without copying it.


LOAD DATA INTO SNOWFLAKE 
************************************************************************************************************************************************

step 1 : create a buckety on s3 location and while creating a bucket make publically accessible so snowflake can will able to use  KEEP BUCKET NAME bucketsnowflakes3.


step 2: create database and tables 

CREATE DATABASE OUR_FIRST_DB; 

CREATE TABLE "OUR_FIRST_DB"."PUBLIC"."LOAN_PAYMENT" (
  "Loan_ID" STRING,
  "loan_status" STRING,
  "Principal" STRING,
  "terms" STRING,
  "effective_date" STRING,
  "due_date" STRING,
  "paid_off_time" STRING,
  "past_due_days" STRING,
  "age" STRING,
  "education" STRING,
  "Gender" STRING);
  
  
 //Check that table is empy
 USE DATABASE OUR_FIRST_DB;

 SELECT * FROM LOAN_PAYMENT;

 
 //Loading the data from S3 bucket
  
 COPY INTO LOAN_PAYMENT
    FROM s3://bucketsnowflakes3/Loan_payments_data.csv
    file_format = (type = csv 
                   field_delimiter = ',' 
                   skip_header=1);
    

//Validate
 SELECT * FROM LOAN_PAYMENT;







WHAT IS DATA WAREHOSUE 
************************************************************************************************************************************************

purpose of warehouse is consolidate and integrate  different datasources and use them and optimize them for reporting and data analysis.
datawarehouse is always a database foor analytics purposes.


                raw data 

hr data       ---------------------->

sales data    ---------------------------------->     ETL PROCESS -----------------> STORE INTO WAREHOUSE --------------------> ANALYSIS 

finanance data ---------------------->


ETL
extract the raw data and store it in seperate database or schema on snowflake , then transformation is to apply busniness logic , and store the trnasformed data into snowflkae 





DIFFERENT LAYES 


    ________________________                        ________________                        ________________
|                            |                 |                        |                |                    |
|           RAW DATA         |                 |    DATA INTEGRATION    |                |   ACCESS LAYER     |
|                            |     --->        |                        |  ---->         |                    |
| __________________________ |                 |    _________________   |                |  ________________  |

STAGING AREA                                     DATA TRANFORMING                         



WHAT IS CLOUD COMPUTING
************************************************************************************************************************************************


Historically it was necessary to have a data center if oragnisation wants to work with the data , to store a large amount of data physically in a data center room,this can cause tedius task since we hahve to maintain infrastructre , reponsible for security server has to be cool which can increase electricity , monitor all this is hectic task.
In snowflake we just have to create account all the stuff will be maintained by the snowflake itself 


coloud computing -
application --> we are only responsible for only  application means a creating databases , tables and just implementing our databases and tables according to our needsand all of the other isuue and responsibilities snowflake is dealing with .

physical machines , vritual machines and physical storage --> the cloud providers are taking care of. so this would be  AWS  , azure of gcp.

oprating system , data , software ---> snowflake deals with oprating system , data , software and metadata .





DIFFRENT EDITIONS OF SNOWFLAKE 
************************************************************************************************************************************************

STANDERD EDITION ------> introducoty level
ENTERPRISE EDITION ------> additional features for needs of large scale enterprises 
BUSINESS CRITICAL EDITION ------> if your organization working with sensitive data this edition is more useful since it provides a high data protection 
VIRTUAL PRIVATE ------> if we are dealing with highly sensitive data for example governance of finincial institutiono then we have virtual private edition, to set this up we nned to directly reach out snowflka team this cannot be setup just by using website since it has our own dedicated environment.



STANDERD EDITION
1.COMPLETE DWH
2.Automatic data encryption
3.broad support for standered and special data type.
4.time travel up to 1 day . this allow us travel back and query historical data that is not anymore available in the database and this can be done upto one day in the past.
5.disaster recovery for 7 days beyond time travel.
6.newwortk policies 
7.secure data share
8.SSO

ENTERPRISE EDITION 
1.multi cluster warehouse 
2.time travel upto 90 days 
3.materialized views
4.search optiization
5.column level security - we can show and hide some specific columns based on the roles of the users.

BUSINESS CRITICAL EDITION
1.all enterprise features.
2.additional security features.
3.support for data specific regulations.
4. database fail over and failback disaster recovery.


virtiual private 
1.all business critical edition 
2.dedicated virtual servers which is complely isolates frmo snowflake accounts.





SNOWFLAKE PRICING 
************************************************************************************************************************************************


TWO BLOCKS OF PRICE IN SNOWFLAKES ARE 

                1.COMPUTE             2.STORAGE 

thsese two main blovks are decoupled means they are independt of each other when we need storage we can simply increase the storage we dont need to pay any additional compute , pay only for what we need snowflake leverages extranal cloud providers like aws, azure .


compute 
Its seperated in three parts 
a)active warehouse ---> we only pay for warehouse which are active it has standerd cost for query processing .
b)cloud services -> for cloud service task like need to use s3 from aws this is mamanged fully by snowflake.
c)serverless --> automatic reclustering , search optimization , and snowpipe . for example in snowpipe we have some data which uploads on s3 and snowflake ppipeline gets triggered after uploading file on s3.

Snowflake Charges 
charged for active warehouse per hour 
billed by seconds - time 
depends on size of warehouse 
chared not in dollars and rupees but in credits in snowflake.
its also depend on the region of your snowflake account 

DATA TRANSFER COSTS
************************************************************************************************************************************************

There is monthly fee depending how much data we stored  so this is calculated ddaily average basis, the cost is based on region of the snowflake account.
we are not charged for uploading the data into snowflake this is because extrnal cloud providers also dont charges the for data ingress but if dhe data out of the snowflake means means data egress then it is charged we have costs for this.
also for storing the data it costs accoring to the data.


Monitor the cost management 
1.go to snowflake acocunt
2.click on admin 
3.click on cost mamagement 

Resource Monitor in Snowflake
************************************************************************************************************************************************

A Resource Monitor in Snowflake is a tool used to track and control how much compute (credits) is used.

Simple Explanation:
What it does: Watches how many credits your virtual warehouses (compute) use.

Why it's useful: Helps you avoid unexpected high costs.

What you can do with it:

Set limits (e.g., 100 credits per day).

Choose actions when a limit is reached:

Notify only

Suspend the warehouse

Suspend immediately

Example:
You create a resource monitor for 200 credits/month. If usage hits 80% (160 credits), it can send a warning. At 100%, it could automatically stop the warehouse to avoid extra charges.


LOADING DATA INTO SNOWFLAKE
************************************************************************************************************************************************

1.BULK LOADING 
a.most frequent method 
b.uses warehouse.
c.loading from stages
d.copy command 
e.transformation possible.


2.CONTINOUS LOADING
a.designed to load small amount of data.
b.Automatically once they are added to the stages 
c.lates result for analysis 
d.Snowpipe (serverless feature)

UNDERSTANDING OF STAGES
************************************************************************************************************************************************

stages in snowflakle is location of data files where data can be laoded from so here stage object we create basically contains the location from where data can be loaded from.
for example AWS bucket  and in the stage object we just store the location so this database object is called stage has properties assosiated to it, one of those properties example location , there can be other properties like the credential so access to the location so we can easily load data from extrnal location.

There are 2 tyoes of stages 
1.External Stages       
Extrnal cloud provider 
s3
google cloud platform 
azure
database object created in schema 
create stage (URL, access settings)


2.Internal Stage
local storage maintained by snowflake 
In Snowflake, stages are locations where data files can be stored (either temporarily or permanently) before being loaded into Snowflake tables or unloaded from them. These stages help manage data movement.




CREATE EXTERNAL STORAGE 
************************************************************************************************************************************************
//STAGE contains all of the necessary information to load data from external source. 
// Database to manage stage objects, fileformats etc.

CREATE OR REPLACE DATABASE MANAGE_DB;

CREATE OR REPLACE SCHEMA external_stages;


// Creating external stage

CREATE OR REPLACE STAGE MANAGE_DB.external_stages.aws_stage
    url='s3://bucketsnowflakes3'
    credentials=(aws_key_id='ABCD_DUMMY_ID' aws_secret_key='1234abcd_key');


// Description of external stage

DESC STAGE MANAGE_DB.external_stages.aws_stage; 
    
    
// Alter external stage   

ALTER STAGE aws_stage
    SET credentials=(aws_key_id='XYZ_DUMMY_ID' aws_secret_key='987xyz');
    
    
// Publicly accessible staging area    

CREATE OR REPLACE STAGE MANAGE_DB.external_stages.aws_stage
    url='s3://bucketsnowflakes3';

// List files in stage

LIST @aws_stage;

//Load data using copy command

COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS
    FROM @aws_stage
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*';




COPY COMMAND TO LOAD DATA INTO 
************************************************************************************************************************************************
// Creating ORDERS table

CREATE OR REPLACE TABLE OUR_FIRST_DB.PUBLIC.ORDERS (
    ORDER_ID VARCHAR(30),
    AMOUNT INT,
    PROFIT INT,
    QUANTITY INT,
    CATEGORY VARCHAR(30),
    SUBCATEGORY VARCHAR(30));
    
SELECT * FROM OUR_FIRST_DB.PUBLIC.ORDERS;
   
// First copy command

COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS
    FROM @aws_stage
    file_format = (type = csv field_delimiter=',' skip_header=1);




// Copy command with fully qualified stage object

COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS
    FROM @MANAGE_DB.external_stages.aws_stage
    file_format= (type = csv field_delimiter=',' skip_header=1);




// List files contained in stage

LIST @MANAGE_DB.external_stages.aws_stage;    




// Copy command with specified file(s)

COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS
    FROM @MANAGE_DB.external_stages.aws_stage
    file_format= (type = csv field_delimiter=',' skip_header=1)
    files = ('OrderDetails.csv');
    



// Copy command with pattern for file names

COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS
    FROM @MANAGE_DB.external_stages.aws_stage
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*';
    

************************************************************************************************************************************************




